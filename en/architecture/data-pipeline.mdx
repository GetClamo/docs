---
title: Data Pipeline
description: Architecture of data flow from CEJ to the database
---

# Data Pipeline

The Clamo data pipeline automates the extraction, normalization, and ingestion of information from Peru's Electronic Justice Center (CEJ - Centro de Justicia Electr√≥nico).

## Pipeline Overview

```mermaid
flowchart TB
    subgraph Discovery [Discovery Layer]
        CejDiscovery[clamo-cej-discovery<br/>Bulk Search]
    end
    
    subgraph Extraction [Extraction Layer]
        CejConnector[clamo-cej-connector<br/>Scraping + LLM]
    end
    
    subgraph EventGateway [Event Gateway]
        ClamoAPI[clamo-api<br/>Event Router]
    end
    
    subgraph Messaging [Message Queue]
        Kafka[(Kafka Topics)]
    end
    
    subgraph Ingestion [Ingestion Layer]
        IngestGo[clamo-ingest-go<br/>Batch Insert]
    end
    
    subgraph Storage [Storage Layer]
        S3[(S3/MinIO<br/>Envelopes)]
        Postgres[(PostgreSQL<br/>Cases DB)]
        Redis[(Redis<br/>Taxonomy Cache)]
    end
    
    CejDiscovery -->|New RUCs| CejConnector
    CejConnector -->|CASE_READY| Kafka
    Kafka --> ClamoAPI
    ClamoAPI -->|cases-ingest| Kafka
    Kafka --> IngestGo
    CejConnector -->|Envelopes| S3
    IngestGo -->|Fetch| S3
    IngestGo -->|COPY| Postgres
    IngestGo -.->|Cache| Redis
```

## Components

### clamo-cej-discovery

**Purpose**: Bulk discovery of new case identifiers (RUCs) from the CEJ.

**Technology**: Python, Playwright, Steel.dev, Temporal

**Key Features**:
- Mass search by correlative numbers (1-99999)
- Intelligent stopping when "not found" rate is high
- Incremental CSV storage to MinIO
- Real-time Ably notifications when cases are found

**Flow**:
1. Iterate through correlative numbers
2. Search CEJ for each correlative
3. Extract basic case metadata
4. Store results in CSV on MinIO
5. Notify downstream systems of new cases

### clamo-cej-connector

**Purpose**: Full case extraction with browser automation and LLM-based normalization.

**Technology**: Python, FastAPI, Playwright, Steel.dev, Temporal, Anthropic (Claude)

**Data Pipeline Stages**:

| Stage | Activity | Description |
|-------|----------|-------------|
| Browser | `cej_browser_search` | Steel session, CAPTCHA solving, HTML capture |
| Parsing | `cej_parse_html` | Convert HTML to `CejHtmlDetail` DTOs |
| Documents | `cej_download_documents` | Download PDFs, LlamaParse extraction |
| LLM | `cej_llm_*` | Summaries, classifications, milestones |
| DB | `cej_ingest_normalized_case` | Upsert via `CejIngestor` |

**Worker Topology**:

```mermaid
flowchart LR
    subgraph Workers [Worker Pods]
        WF[Workflow Workers<br/>cej-coordinator<br/>cej-expediente]
        BR[Browser Workers<br/>cej-browser]
        LLM[LLM Workers<br/>cej-llm]
        ING[Ingest Workers<br/>cej-ingest]
    end
    
    subgraph Queues [Task Queues]
        Q1[cej-coordinator]
        Q2[cej-expediente]
        Q3[cej-browser]
        Q4[cej-llm]
        Q5[cej-ingest]
    end
    
    WF --> Q1
    WF --> Q2
    BR --> Q3
    LLM --> Q4
    ING --> Q5
```

**Temporal Workflows**:
- `CejCaseJobWorkflow` - Top-level orchestrator, spawns child workflows
- `CejExpedienteWorkflow` - Full extraction flow for a single case

### clamo-ingest-go

**Purpose**: High-performance batch database insertion.

**Technology**: Go 1.25+, Kafka, Temporal, PostgreSQL (pgx), Redis

**Performance Optimizations**:
- PostgreSQL `COPY FROM` for bulk inserts (10-100x faster)
- Redis taxonomy cache reduces DB queries by 90%
- Connection pooling (20 max connections)
- Automatic retries with exponential backoff

**Data Flow**:

```mermaid
sequenceDiagram
    participant CEJ as clamo-cej-connector
    participant Kafka as Kafka
    participant API as clamo-api
    participant Ingest as clamo-ingest-go
    participant S3 as S3/MinIO
    participant DB as PostgreSQL
    
    CEJ->>Kafka: CASE_READY (cej-events)
    Kafka->>API: Consume event
    API->>API: Validate + audit
    API->>Kafka: Publish (cases-ingest)
    Kafka->>Ingest: Consume event
    Ingest->>S3: Fetch envelope
    S3-->>Ingest: Case data
    Ingest->>DB: COPY batch insert
    Note over DB: Cases, Progress,<br/>Movements, Milestones
```

**Inserted Tables**:
- `Cases` - UPSERT on expediente + company_id
- `CaseProgress` - Stage/substage timeline
- `Movements` - Case file actions
- `StatusSnapshots` - Point-in-time status
- `CaseMilestones` - Important events

## LLM Normalization Pipeline

The connector uses Anthropic Claude for multi-stage normalization:

```mermaid
flowchart LR
    subgraph Input [Raw Data]
        HTML[CEJ HTML]
        PDFs[PDF Documents]
    end
    
    subgraph LLM [LangChain Stages]
        DocSum[Document<br/>Summaries]
        MovClass[Movement<br/>Classification]
        MileMap[Milestone<br/>Mapping]
        Timeline[Timeline<br/>Segmentation]
        Final[Final Payload<br/>Synthesis]
    end
    
    subgraph Output [Normalized]
        Envelope[Case Envelope]
    end
    
    HTML --> DocSum
    PDFs --> DocSum
    DocSum --> MovClass
    MovClass --> MileMap
    MileMap --> Timeline
    Timeline --> Final
    Final --> Envelope
```

**LLM Stages**:
1. **Document Summaries** - Extract key information from PDFs
2. **Movement Classification** - Categorize case actions
3. **Milestone Mapping** - Identify important events
4. **Timeline Segmentation** - Split into stages/substages
5. **Final Synthesis** - Produce normalized envelope

## Caching Strategy

The CEJ connector implements multi-level caching:

| Setting | Default | Description |
|---------|---------|-------------|
| `CACHE_ENABLED` | `true` | Enable/disable caching |
| `CACHE_BACKEND` | `memory` | `memory` or `redis`/`valkey` |
| `CACHE_TTL_SECONDS` | `1200` | 20 minutes default TTL |
| `CACHE_FORCE_REFRESH_PARAM` | `force_refresh` | Query param to bypass cache |

Cached artifacts include:
- Browser session HTML
- Parsed case details
- Downloaded documents
- LLM normalization results

## Observability

### Tracing & Logging
- **Logfire** - Spans from FastAPI + Temporal activities
- **LangSmith** - LangChain stage tracing (summaries, classifications)

### Metrics
- Per-stage timings: `browser_time`, `http_time`, `llm_time`, `db_time`
- Queue depth for load shedding
- Workflow task execution latency (target less than 1s)

### Search Attributes (Temporal)
- `TenantId` - Company identifier
- `ExpedienteId` - Case identifier
- `ForceRefresh` - Cache bypass flag
- `CacheHit` - Cache hit/miss
- `MovementCount` - Number of movements

## Scaling Recommendations

For 150 concurrent job submissions:
- ~5 workflow pods
- ~15 activity pods (split across browser/LLM/ingest)
- All workers use same container image
- `WORKER_ROLE` env var determines queue assignment

## Related Documentation

- [clamo-cej-connector Service](/en/services/clamo-cej-connector)
- [clamo-cej-discovery Service](/en/services/clamo-cej-discovery)
- [clamo-ingest-go Service](/en/services/clamo-ingest-go)
